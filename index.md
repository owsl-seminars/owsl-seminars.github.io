---
title: Statistical Learning Seminars
layout: page
feature_image:
feature_text:
---

The effects of CoViD-19 pervade through research communities across the
globe, causing canceled conferences, postponed research visits, and suspended
projects. Like [many others](/links), we have sought other opportunities for
collaboration in spite of the current state of affairs and have therefore
organized this online seminar series in statistical learning.

## Format

We use [zoom](https://zoom.us/) for all the sessions. Upon joining the seminar,
you will be placed in a waiting room; please wait for the host to let you in to
the meeting.

The seminars are approximately an hour long with anywhere between 20 and 40
minutes allocated to the presentation and the rest for discussion. Sessions
are held on a regular basis on Fridays at [15:30
CET](https://www.thetimezoneconverter.com/?t=15%3A30%20pm&tz=Stockholm&). See
[Previous Talks](/previous-talks) for recordings, slides, and resources from
previous seminars.

### Zoom Link

<https://lu-se.zoom.us/j/65067339175>

### Mailing List

To receive announcements for upcoming seminars, please join the group at
<https://groups.google.com/g/statlearnsem>.

### Calendar Event

[Link to calendar event](https://lu-se.zoom.us/meeting/u5Etce6rrTIrHdGmDxIUKT33_HsILcrt6Tui/ics?icsToken=98tyKu-trj0tGdecsR6CR_MMAo_oKOnztlhcgqd6kTv9KhV4VlClCcpRG558AsyG)

## Upcoming Talks

### October 15, [15:30 CET][tz]

#### Olof Zetterqvist (University of Gothenburg/Chalmers)

Title
: Entropy Weighted Regularisation: A General Way to Debias Regularisation
Penalties

Abstract
: Lasso and ridge regression are well established and successful models for
variance reduction and, for the lasso, variable selection. However, they come
with a disadvantage of an increased bias in the estimator. In this seminar, I
will talk about our general method that learns individual weights for each term
in the regularisation penalty (e.g. lasso or ridge) with the goal to reduce the
bias. To bound the amount of freedom for the model to choose the weights, a new
regularisation term, that imposes a cost for choosing small weights, is
introduced. If the form of this term is chosen wisely, the apparent doubling of
the number of parameters vanishes, by means of solving for the weights in terms
of the parameter estimates. We show that these estimators potentially keep the
original estimators' fundamental properties and experimentally verify that this
can indeed reduce bias.

[tz]: https://www.thetimezoneconverter.com/?t=15%3A30%20pm&tz=Stockholm&

## Organization

This seminar series is a joint effort organized by
[The Department of Mathematics, Wroc≈Çaw University](https://www.math.uni.wroc.pl),
[The Department of Mathematics, University of Burgundy](https://math.u-bourgogne.fr/), and
[The Department of Statistics, Lund University](https://stat.lu.se).

<div class="row">
  <div class="column">
    <img src="assets/logo-lu.svg" alt="Lund University" style="height:170px">
  </div>
  <div class="column">
    <img src="assets/logo-burgundy.png" alt="University of Burgundy" style="width:auto height:170px">
  </div>
  <div class="column">
    <img src="assets/logo-wroclaw.svg" alt="Wroclaw University" style="height:170px">
  </div>
</div>
